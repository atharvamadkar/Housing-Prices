# -*- coding: utf-8 -*-
"""House.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10xfHRa46l3IEPlI51JgMq43vr-eULOOI
"""

#from google.colab import files

#uploaded = files.upload()

# Commented out IPython magic to ensure Python compatibility.
#Importing required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
plt.rcParams['figure.figsize'] = (10,8)
import seaborn as sns
from scipy import stats
from scipy.stats import norm

#Loading data
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

"""After loading the data into our variables, we'll have a look at our data."""

train.head()

test.head()

print(f"The train data has {train.shape[0]} rows and {train.shape[1]} columns.")
print('--------------------')
print(f"The test data has {test.shape[0]} rows and {test.shape[1]} columns.")

#Checking if the dataset has any missing values
train.columns[train.isnull().any()]

"""Out of 81 features, 19 features have missing values. Let's check the percentage of missing values in these columns."""

#missing value counts in each of these columns
miss = train.isnull().sum()/len(train)
miss = miss[miss>0]
miss.sort_values(inplace=True)
miss

"""We can infer that PoolQC has 99.5% values missing followed by MiscFeature, Alley and Fence. Let's plot a bar plot explaining these missing values."""

#Visualize missing values
miss = miss.to_frame()
miss.columns = ['count']
miss.index.names = ['Name']
miss['Name'] = miss.index

#plot the missing value count
sns.set(style='whitegrid',color_codes=True)
sns.barplot(x='Name',y='count',data=miss)
plt.xticks(rotation =90)
plt.show()

"""Checking the distribution of the target variable."""

#SalePrice
sns.distplot(train['SalePrice'])

"""SalePrice is the target variable , has a right-skewed distribution. A normally distributed (or close to normal) target variable helps in better modeling the relationship between target and independent variables. For this, we'll take the log transform of this variable so that it becomes normally distributed. In addition, linear algortihms assume constant variance in the error term.

Alternatively, we can also cofirm this skewed behaviour using skewness metric
"""

#skewness
print('The skewness of SalePrice is {}'.format(train['SalePrice'].skew()))

"""Now taking the log transform of thsi variable and checking it's skewness again i.e if it is any closer to the normal."""

#Transforming the target variable
target = np.log(train['SalePrice'])
print('Skewness is ',target.skew())

sns.distplot(target)

"""Log transformation of the target variable has made it's distribution more normal. As we have 80 variables , visualizing them one by oen wouldn't be adivisable.Instead, we'll look at variables based on their correlation with the target variable.
Also, we'll seperate the numeric and categorical vairables and explore this data in a different angle.
"""

#seperate variables into a new dataframe
numeric_data = train.select_dtypes(include =[np.number])
cat_data = train.select_dtypes(exclude =[np.number])

print(f'There are {len(numeric_data)} numeric and {len(cat_data)} categorical columns in train data.')

del numeric_data['Id']

"""Checking for the correlation behaviour of the numeric variables. If found, we can later remove these correlated variables as they won't provide any useful information to our model."""

#correlation plot
corr = numeric_data.corr()
sns.heatmap(corr)

"""Observe the last row of the correlation map. We can see that some variables are strongly correlated with the target variable than others.

Printing numeric correlation score that'll give a better understanding of the situation.
"""

print(corr['SalePrice'].sort_values(ascending=False)[:15],'\n') #Top 15
print('-------------------\n')
print(corr['SalePrice'].sort_values(ascending=False)[-5:],'\n') #Last 5

"""OverallQual is 79% related to SalePrice. OverallQual is the quality of material used in building the house, and people definitely consider it while buying their 'dream house'.

Let's the OverallQual variable in detail.
"""

train['OverallQual'].unique()

"""OverallQual is an index measured on a scale from 1-10. We can treat this as an ordinary variable as inherent order matters.

Checking median sale price of houses wrt OverallQual.
"""

#Checking mean price per quality and plotting it
pivot = train.pivot_table(index='OverallQual',values='SalePrice',aggfunc=np.median)

pivot

"""Let's understand the behaviour by plotting the bar graph of this data."""

pivot.plot(kind='bar',color='blue')

"""This behaviour is normal. 
Now let's visualize the next correlated variable GrLivArea and understand it's behaviour.
"""

#GrLivArea plot
sns.jointplot(x=train['GrLivArea'],y = train['SalePrice'])

"""As one can observe that SalePrice is directly correlated with GrLivArea, although we can spot an outlier for GrLivArea >5000. So we'll remove that as outliers can damage the efficiency of our model.

Now checking out the categorical variables.
"""

cat_data.describe()

"""Checking the median sale price of a house based on SaleCondition(it tells the condition of the sale, though not much info is given about it's categories)."""

sp_pivot = train.pivot_table(index='SaleCondition',values='SalePrice',aggfunc=np.median)
sp_pivot

sp_pivot.plot(kind='bar',color='magenta')

"""SaleCondition Partial ha slightly higher mean sale price. But as we don't much info about it, we cannot generate any good insight from the above data.

Now we'll use the ANOVA test to understand the correlation between categorical variables and SalePrice.
"""

cat = [f for f in train.columns if train.dtypes[f]=='object']

def anova(frame):
  anv = pd.DataFrame()
  anv['features'] = cat
  pvals = []
  for c in cat:
    samples = []
    for cls in frame[c].unique():
      s = frame[frame[c]==cls]['SalePrice'].values
      samples.append(s)
    pval = stats.f_oneway(*samples)[1]
    pvals.append(pval)
  anv['pval'] = pvals
  return anv.sort_values('pval')

cat_data['SalePrice'] = train.SalePrice.values
k = anova(cat_data)
k['disparity'] = np.log(1/k['pval'].values)

sns.barplot(data=k,x='features',y='disparity')
plt.xticks(rotation =90)

"""Here can observe the Neighbourhood, f.b ExterQual, KitchenQual are also considered by the people while buying new house.

Plotting Histogram for all numeric values to see if all values are skewed.
For categorical variable we'll create the boxplot.
"""

#create  numeric plots
num = [f for f in train.columns if train.dtypes[f] !='object']
num.remove('Id')

nd = pd.melt(train,value_vars=num)
n1 = sns.FacetGrid(nd,col='variable',col_wrap=4,sharex=False,sharey=False)
n1 = n1.map(sns.distplot,'value')
n1

"""One can observe that most of the plots are right skewed. We'll transform later.

No we'll create boxplot for the categorical data.
"""

def boxplot(x,y,**kwargs):
  sns.boxplot(x=x,y=y)
  x = plt.xticks(rotation=90)

cat = [f for f in train.columns if train.dtypes[f] =='object']

p = pd.melt(train,id_vars='SalePrice',value_vars=cat)
g = sns.FacetGrid(p,col='variable',col_wrap=2,sharex=False,sharey=False,size=5)
g = g.map(boxplot,'value','SalePrice')
g

"""We can see that most of our variables contain outliers. It'll take us days to clean them up so we'll leave it as it is and let our model deal with it.
Usually tree based algorithms are robust to outliers.

**DATA PREPROCESSING**

Now we'll deal with some outliers, encode variables, impute missing values, and take every possible step to remove inconsistencies from the data set.
"""

#removing outliers
train.drop(train[train['GrLivArea']>4000].index,inplace=True)
train.shape #removed 4 rows

stats.mode(test['GarageYrBlt']).mode

#Correcting row 666
test.loc[666,'GarageQual'] = 'TA' #stats.mode(test['GarageQual']).mode
test.loc[666,'GarageCond'] = 'TA' #stats.mode(test['GarageCond']).mode
test.loc[666,'GarageFinish'] = 'Unf' #stats.mode(test['GarageFinish']).mode
test.loc[666,'GarageYrBlt'] = '1980'    #stats.mode(test['GarageYrBlt']).mode

#row 1116, mark as missing
test.loc[1116,'GarageType'] = np.nan

"""Encoding categorical variables."""

#importing necessary functions
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

def factorize(data,var,fill_na=None):
  if fill_na is not None:
    data[var].fillna(fill_na,inplace=True)
  le.fit(data[var])
  data[var] = le.transform(data[var])
  return data

"""The above function imputes blank levels with mean values which we have input manually.

Now let's impute the missingvalues in LotFrontage variable using median value of LotFrontage by Neighbourhood. Imputation strategies are built during data exploration.
We'll combine the test and train dataset so that we can modify them together.
"""

#Combining the datasets
alldata = train.append(test)
alldata.shape

#Imputing LotFrontage
lot_fr_by_neigh = train['LotFrontage'].groupby(train['Neighborhood'])

for key,group in lot_fr_by_neigh:
  idx =(alldata['Neighborhood']==key) & (alldata['LotFrontage'].isnull())
  alldata.loc[idx,'LotFrontage'] = group.median()

"""Now we'll impute the missing values in the numerical variables."""

alldata['MasVnrArea'].fillna(0,inplace=True)
alldata['BsmtFinSF1'].fillna(0,inplace=True)
alldata['BsmtFinSF2'].fillna(0,inplace=True)
alldata['BsmtUnfSF'].fillna(0,inplace=True)
alldata['TotalBsmtSF'].fillna(0,inplace=True)
alldata['GarageArea'].fillna(0,inplace=True)
alldata["BsmtFullBath"].fillna(0, inplace=True)
alldata["BsmtHalfBath"].fillna(0, inplace=True)
alldata["GarageCars"].fillna(0, inplace=True)
alldata["GarageYrBlt"].fillna(0.0, inplace=True)
alldata["PoolArea"].fillna(0, inplace=True)

"""Variables having 'qual' in them can be treated as normal variable as discussed above. Now we'll convert the categorical into ordinary variables."""

qual_dict ={np.nan:0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5}
name = np.array(['ExterQual','PoolQC' ,'ExterCond','BsmtQual','BsmtCond','HeatingQC','KitchenQual','FireplaceQu', 'GarageQual','GarageCond'])

for i in name:
  alldata[i] = alldata[i].map(qual_dict).astype(int)

alldata['BsmtExposure'] = alldata['BsmtExposure'].map({np.nan: 0, "No": 1, "Mn": 2, "Av": 3, "Gd": 4}).astype(int)

bsmt_fin_dict = {np.nan: 0, "Unf": 1, "LwQ": 2, "Rec": 3, "BLQ": 4, "ALQ": 5, "GLQ": 6}
alldata['BsmtFinType1'] = alldata['BsmtFinType1'].map(bsmt_fin_dict).astype(int)
alldata['BsmtFinType2'] = alldata['BsmtFinType2'].map(bsmt_fin_dict).astype(int)
alldata['Functional'] = alldata['Functional'].map({np.nan: 0, "Sal": 1, "Sev": 2, "Maj2": 3, "Maj1": 4, "Mod": 5, "Min2": 6, "Min1": 7, "Typ": 8}).astype(int)

alldata['GarageFinish'] = alldata['GarageFinish'].map({np.nan: 0, "Unf": 1, "RFn": 2, "Fin": 3}).astype(int)
alldata['Fence'] = alldata['Fence'].map({np.nan: 0, "MnWw": 1, "GdWo": 2, "MnPrv": 3, "GdPrv": 4}).astype(int)

#encoding data
alldata['CentralAir'] = (alldata['CentralAir']=='Y')*1.0
varst = np.array(['MSSubClass','LotConfig','Neighborhood','Condition1','BldgType','HouseStyle','RoofStyle','Foundation','SaleCondition'])

for x in varst:
  factorize(alldata,x)

#encoding variables and imputing missing values
alldata = factorize(alldata,'MSZoning','RL')
alldata = factorize(alldata,'Exterior1st','RL')
alldata = factorize(alldata,'Exterior2nd','RL')
alldata = factorize(alldata,'MasVnrType','RL')
alldata = factorize(alldata,'SaleType','RL')

"""# **Feature Engineering**"""

#Creating New variables (1 or 0) based on irregular counts
#The level with highest count is kept as 1 and rest 0
alldata['IsRegularLotShape'] = (alldata['LotShape']=='Reg')*1
alldata['IsLandLevel'] = (alldata['LandContour']=='Lvl')*1
alldata['IsLandSlopeGentle'] = (alldata['LandSlope']=='Gtl')*1
alldata['IsElectricalSBrkr'] = (alldata['Electrical']=='SBrkr')*1
alldata['IsGarageDetached'] = (alldata['GarageType']=='Detchd')*1
alldata['IsPavedDrive'] = (alldata['PavedDrive']=='Y')*1
alldata['hasShed'] = (alldata['MiscFeature']=='Shed')*1
alldata['Remodeled'] = (alldata['YearRemodAdd']!=alldata['YearBuilt'])*1

#Modeling during Sale year
alldata['RecentRemodel'] = (alldata['YearRemodAdd']==alldata['YrSold'])*1

#Was this house sold in the year it was built
alldata["VeryNewHouse"] = (alldata["YearBuilt"] == alldata["YrSold"]) * 1
alldata["Has2ndFloor"] = (alldata["2ndFlrSF"] == 0) * 1
alldata["HasMasVnr"] = (alldata["MasVnrArea"] == 0) * 1
alldata["HasWoodDeck"] = (alldata["WoodDeckSF"] == 0) * 1
alldata["HasOpenPorch"] = (alldata["OpenPorchSF"] == 0) * 1
alldata["HasEnclosedPorch"] = (alldata["EnclosedPorch"] == 0) * 1
alldata["Has3SsnPorch"] = (alldata["3SsnPorch"] == 0) * 1
alldata["HasScreenPorch"] = (alldata["ScreenPorch"] == 0) * 1

#setting levels with high count as 1 and the rest as 0
#you can check for them using the value_counts function
alldata["HighSeason"] = alldata["MoSold"].replace({1: 0, 2: 0, 3: 0, 4: 1, 5: 1, 6: 1, 7: 1, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0})
alldata["NewerDwelling"] = alldata["MSSubClass"].replace({20: 1, 30: 0, 40: 0, 45: 0,50: 0, 60: 1, 70: 0, 75: 0, 80: 0, 85: 0,90: 0, 120: 1, 150: 0, 160: 0, 180: 0, 190: 0})

alldata.shape

"""Now we have added 19 more features to our data.

Let's create some more features.
"""

#We'll create alldata2 and use this as a reference
alldata2 = train.append(test)

alldata["SaleCondition_PriceDown"] = alldata2.SaleCondition.replace({'Abnorml': 1, 'Alloca': 1, 'AdjLand': 1, 'Family': 1, 'Normal': 0, 'Partial': 0})

#House ready before the sale or not
#alldata["BoughtOffPlan"] = alldata2.SaleCondition.replace({"Abnorml" : 0, "Alloca" : 0, "AdjLand" : 0, "Family" : 0, "Normal" : 0, "Partial" : 1})

#alldata["BadHeating"] = alldata2.HeatingQC.replace({'Ex': 0, 'Gd': 0, 'TA': 0, 'Fa': 1, 'Po': 1})

"""We've several categories like Garage associated with the area of the property. An useful variable would be the sum of all these areas."""

#Calculating total area
area_cols = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 
             'GrLivArea', 'GarageArea', 'WoodDeckSF','OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'LowQualFinSF', 'PoolArea' ]

alldata["TotalArea"] = alldata[area_cols].sum(axis=1)
alldata["TotalArea1st2nd"] = alldata["1stFlrSF"] + alldata["2ndFlrSF"]

#Age of house (2010 data)
alldata["Age"] = 2010 - alldata["YearBuilt"]

alldata["TimeSinceSold"] = 2010 - alldata["YrSold"]

#Season
alldata["SeasonSold"] = alldata["MoSold"].map({12:0, 1:0, 2:0, 3:1, 4:1, 5:1, 6:2, 7:2, 8:2, 9:3, 10:3, 11:3}).astype(int)

alldata["YearsSinceRemodel"] = alldata["YrSold"] - alldata["YearRemodAdd"]

#Simplifying existing features into good/average/bad based on number of counts
alldata["SimplOverallQual"] = alldata.OverallQual.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2, 7 : 3, 8 : 3, 9 : 3, 10 : 3})
alldata["SimplOverallCond"] = alldata.OverallCond.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2, 7 : 3, 8 : 3, 9 : 3, 10 : 3})
alldata["SimplPoolQC"] = alldata.PoolQC.replace({1 : 1, 2 : 1, 3 : 2, 4 : 2})
alldata["SimplGarageCond"] = alldata.GarageCond.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})
alldata["SimplGarageQual"] = alldata.GarageQual.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})
alldata["SimplFireplaceQu"] = alldata.FireplaceQu.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})
#alldata["SimplFireplaceQu"] = alldata.FireplaceQu.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})
alldata["SimplFunctional"] = alldata.Functional.replace({1 : 1, 2 : 1, 3 : 2, 4 : 2, 5 : 3, 6 : 3, 7 : 3, 8 : 4})
alldata["SimplKitchenQual"] = alldata.KitchenQual.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})
alldata["SimplHeatingQC"] = alldata.HeatingQC.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})
alldata["SimplBsmtFinType1"] = alldata.BsmtFinType1.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2})
alldata["SimplBsmtFinType2"] = alldata.BsmtFinType2.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2})
alldata["SimplBsmtCond"] = alldata.BsmtCond.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})
alldata["SimplBsmtQual"] = alldata.BsmtQual.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})
alldata["SimplExterCond"] = alldata.ExterCond.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})
alldata["SimplExterQual"] = alldata.ExterQual.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})

#Group neighborhood variables based on the following variables
train['SalePrice'].groupby(train['Neighborhood']).median().sort_values().plot(kind='bar')

"""Using hints from the above graph , we'll combine levels of neighborhood variable into fewer levels."""

neighborhood_map = {"MeadowV" : 0, "IDOTRR" : 1, "BrDale" : 1, "OldTown" : 1, "Edwards" : 1, "BrkSide" : 1, "Sawyer" : 1, "Blueste" : 1,
                    "SWISU" : 2, "NAmes" : 2, "NPkVill" : 2, "Mitchel" : 2, "SawyerW" : 2, "Gilbert" : 2, "NWAmes" : 2, "Blmngtn" : 2, "CollgCr" : 2,
                    "ClearCr" : 3, "Crawfor" : 3, "Veenker" : 3, "Somerst" : 3, "Timber" : 3, "StoneBr" : 4, "NoRidge" : 4, "NridgHt" : 4}

alldata['NeighborhoodBin'] = alldata2['Neighborhood'].map(neighborhood_map)
alldata.loc[alldata2.Neighborhood == 'NridgHt', "Neighborhood_Good"] = 1
alldata.loc[alldata2.Neighborhood == 'Crawfor', "Neighborhood_Good"] = 1
alldata.loc[alldata2.Neighborhood == 'StoneBr', "Neighborhood_Good"] = 1
alldata.loc[alldata2.Neighborhood == 'Somerst', "Neighborhood_Good"] = 1
alldata.loc[alldata2.Neighborhood == 'NoRidge', "Neighborhood_Good"] = 1
alldata["Neighborhood_Good"].fillna(0, inplace=True)
alldata["SaleCondition_PriceDown"] = alldata2.SaleCondition.replace({'Abnorml': 1, 'Alloca': 1, 'AdjLand': 1, 'Family': 1, 'Normal': 0, 'Partial': 0})

#House ready before sale or not
alldata["BoughtOffPlan"] = alldata2.SaleCondition.replace({"Abnorml" : 0, "Alloca" : 0, "AdjLand" : 0, "Family" : 0, "Normal" : 0, "Partial" : 1})
alldata["BadHeating"] = alldata2.HeatingQC.replace({'Ex': 0, 'Gd': 0, 'TA': 0, 'Fa': 1, 'Po': 1})

alldata.shape

"""Now we've many new features.
Let's split the data into train and test set.
"""

#Splitting data
train_new = alldata[alldata['SalePrice'].notnull()]
test_new = alldata[alldata['SalePrice'].isnull()]

print ('Train', train_new.shape)
print ('--------------------')
print ('Test', test_new.shape)

"""Now is the time to transform numeric features and remove it's skewness."""

#Getting numeric features
numeric_features = [f for f in train_new.columns if train_new[f].dtype !='object']

#Transforming using log(1+x)
from scipy.stats import skew

skewed = train_new[numeric_features].apply(lambda x: skew(x.dropna().astype(float)))
skewed = skewed[skewed>0.75]
skewed = skewed.index
train_new[skewed] = np.log1p(train_new[skewed])
test_new[skewed] = np.log1p(test_new[skewed])

del test_new['SalePrice']

"""Standardizing Numeric features."""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

scaler.fit(train_new[numeric_features])
scaled = scaler.transform(train_new[numeric_features])

for i,col in enumerate(numeric_features):
  train_new[col] = scaled[:,i]

numeric_features.remove('SalePrice')
scaled = scaler.fit_transform(test_new[numeric_features])

for i, col in enumerate(numeric_features):
  test_new[col] = scaled[:,i]

"""Time to OneHotEncode categorical variables."""

def onehot(onehot_df,df,column_name,fill_na):
  onehot_df[column_name] =df[column_name]
  if fill_na is not None:
    onehot_df.fillna(fill_na,inplace=True)

  dummies = pd.get_dummies(onehot_df[column_name],prefix ='_'+column_name)
  onehot_df = onehot_df.join(dummies)
  onehot_df = onehot_df.drop([column_name],axis=1)
  return onehot_df

def munge_onehot(df):
       onehot_df = pd.DataFrame(index = df.index)

       onehot_df = onehot(onehot_df, df, "MSSubClass", None)
       onehot_df = onehot(onehot_df, df, "MSZoning", "RL")
       onehot_df = onehot(onehot_df, df, "LotConfig", None)
       onehot_df = onehot(onehot_df, df, "Neighborhood", None)
       onehot_df = onehot(onehot_df, df, "Condition1", None)
       onehot_df = onehot(onehot_df, df, "BldgType", None)
       onehot_df = onehot(onehot_df, df, "HouseStyle", None)
       onehot_df = onehot(onehot_df, df, "RoofStyle", None)
       onehot_df = onehot(onehot_df, df, "Exterior1st", "VinylSd")
       onehot_df = onehot(onehot_df, df, "Exterior2nd", "VinylSd")
       onehot_df = onehot(onehot_df, df, "Foundation", None)
       onehot_df = onehot(onehot_df, df, "SaleType", "WD")
       onehot_df = onehot(onehot_df, df, "SaleCondition", "Normal")

       #Fill in missing MasVnrType for rows that do have a MasVnrArea.
       temp_df = df[["MasVnrType", "MasVnrArea"]].copy()
       idx = (df["MasVnrArea"] != 0) & ((df["MasVnrType"] == "None") | (df["MasVnrType"].isnull()))
       temp_df.loc[idx, "MasVnrType"] = "BrkFace"
       onehot_df = onehot(onehot_df, temp_df, "MasVnrType", "None")

       onehot_df = onehot(onehot_df, df, "LotShape", None)
       onehot_df = onehot(onehot_df, df, "LandContour", None)
       onehot_df = onehot(onehot_df, df, "LandSlope", None)
       onehot_df = onehot(onehot_df, df, "Electrical", "SBrkr")
       onehot_df = onehot(onehot_df, df, "GarageType", "None")
       onehot_df = onehot(onehot_df, df, "PavedDrive", None)
       onehot_df = onehot(onehot_df, df, "MiscFeature", "None")
       onehot_df = onehot(onehot_df, df, "Street", None)
       onehot_df = onehot(onehot_df, df, "Alley", "None")
       onehot_df = onehot(onehot_df, df, "Condition2", None)
       onehot_df = onehot(onehot_df, df, "RoofMatl", None)
       onehot_df = onehot(onehot_df, df, "Heating", None)

       # we'll have these as numerical variables too
       onehot_df = onehot(onehot_df, df, "ExterQual", "None")
       onehot_df = onehot(onehot_df, df, "ExterCond", "None")
       onehot_df = onehot(onehot_df, df, "BsmtQual", "None")
       onehot_df = onehot(onehot_df, df, "BsmtCond", "None")
       onehot_df = onehot(onehot_df, df, "HeatingQC", "None")
       onehot_df = onehot(onehot_df, df, "KitchenQual", "TA")
       onehot_df = onehot(onehot_df, df, "FireplaceQu", "None")
       onehot_df = onehot(onehot_df, df, "GarageQual", "None")
       onehot_df = onehot(onehot_df, df, "GarageCond", "None")
       onehot_df = onehot(onehot_df, df, "PoolQC", "None")
       onehot_df = onehot(onehot_df, df, "BsmtExposure", "None")
       onehot_df = onehot(onehot_df, df, "BsmtFinType1", "None")
       onehot_df = onehot(onehot_df, df, "BsmtFinType2", "None")
       onehot_df = onehot(onehot_df, df, "Functional", "Typ")
       onehot_df = onehot(onehot_df, df, "GarageFinish", "None")
       onehot_df = onehot(onehot_df, df, "Fence", "None")
       onehot_df = onehot(onehot_df, df, "MoSold", None)

       # Divide  the years between 1871 and 2010 into slices of 20 years
       year_map = pd.concat(pd.Series("YearBin" + str(i+1), index=range(1871+i*20,1891+i*20))  for i in range(0, 7))
       yearbin_df = pd.DataFrame(index = df.index)
       yearbin_df["GarageYrBltBin"] = df.GarageYrBlt.map(year_map)
       yearbin_df["GarageYrBltBin"].fillna("NoGarage", inplace=True)
       yearbin_df["YearBuiltBin"] = df.YearBuilt.map(year_map)
       yearbin_df["YearRemodAddBin"] = df.YearRemodAdd.map(year_map)

       onehot_df = onehot(onehot_df, yearbin_df, "GarageYrBltBin", None)
       onehot_df = onehot(onehot_df, yearbin_df, "YearBuiltBin", None)
       onehot_df = onehot(onehot_df, yearbin_df, "YearRemodAddBin", None)
       return onehot_df

#Creating OneHot features
onehot_df = munge_onehot(train)

neighborhood_train = pd.DataFrame(index=train_new.shape)
neighborhood_train['NeighborhoodBin'] = train_new['NeighborhoodBin']
neighborhood_test = pd.DataFrame(index=test_new.shape)
neighborhood_test['NeighborhoodBin'] = test_new['NeighborhoodBin']

onehot_df = onehot(onehot_df, neighborhood_train, 'NeighborhoodBin', None)

train_new.shape

onehot_df.shape

#Adding one-hot variables in our train data set.
train_new = train_new.join(onehot_df) 
train_new.shape

"""Now we have 433 columns!!!

Let's the same for the test data.
"""

#Creating OneHot features for test data
onehot_df_te = munge_onehot(test)
onehot_df_te = onehot(onehot_df_te, neighborhood_test, "NeighborhoodBin", None)
test_new = test_new.join(onehot_df_te)
test_new.shape

"""Difference in number of columns is due the fact that some of the features are not available in the test set. We'll remove those columns from train set to make number of columns equal."""

train_new.columns

#dropping columns from train set
drop_cols = ["_Exterior1st_ImStucc", "_Exterior1st_Stone","_Exterior2nd_Other","_HouseStyle_2.5Fin","_RoofMatl_Membran", "_RoofMatl_Metal",
             "_RoofMatl_Roll", "_Condition2_RRAe", "_Condition2_RRAn", "_Condition2_RRNn", "_Heating_Floor", "_Heating_OthW", "_Electrical_Mix",
             "_MiscFeature_TenC", "_GarageQual_Ex",  "_PoolQC_Fa"]
train_new.drop(drop_cols, axis=1, inplace=True)
train_new.shape

"""Removing moree columns.
One's which has huge number of zeros, or secondly features which are not present in both sets.
"""

#removing one column missing from train data
#test_new.drop(["_MSSubClass_150"], axis=1, inplace=True)

# Dropping these columns
drop_cols = ["_Condition2_PosN", # only two are not zero
         "_MSZoning_C (all)",
         "_MSSubClass_160"]

train_new.drop(drop_cols, axis=1, inplace=True)
test_new.drop(drop_cols, axis=1, inplace=True)

test_new.drop(["_MSSubClass_150"], axis=1, inplace=True)

"""Transforming target variable and storing it in new array."""

#Creating a label set
label_df = pd.DataFrame(index = train_new.index, columns = ['SalePrice'])
label_df['SalePrice'] = np.log(train['SalePrice'])
print("Training set size:", train_new.shape)
print("Test set size:", test_new.shape)

"""## **Model Training and Evaluation**

Our data is now ready. 
Let's starting training our models.
We'll use 3 algorithms
 
1) XGBoost
2) Nueral Network
3) Lasso Regression
"""

import xgboost as xgb
regr = xgb.XGBRegressor(colsample_bytree=0.2,
                       gamma=0.0,
                       learning_rate=0.05,
                       max_depth=6,
                       min_child_weight=1.5,
                       n_estimators=7200,
                       reg_alpha=0.9,
                       reg_lambda=0.6,
                       subsample=0.2,
                       seed=42,
                       silent=1)

#regr.fit(train_new, label_df)

"""One can experiment with these hyperparameters."""

num = ['Alley', 'Condition2', 'Electrical', 'GarageType', 'GarageYrBlt', 'Heating', 'LandContour', 'LandSlope', 'LotShape',
       'MiscFeature', 'PavedDrive', 'RoofMatl', 'Street', 'Utilities']
      

train_new = train_new.drop(num,axis =1)

test_new = test_new.drop(num,axis=1)

train_new['_NeighnorhoodBin']= train_new['_NeighborhoodBin_-0.04760808471778121']
train_new = train_new.drop(['_NeighborhoodBin_-0.04760808471778121'],axis=1)

test_new['_NeighnorhoodBin']= test_new['_NeighborhoodBin_-1.0875767879360991']
test_new = test_new.drop(['_NeighborhoodBin_-1.0875767879360991'],axis=1)

X2 = train_new.drop(['SalePrice'],axis=1)

regr.fit(X2, label_df)

"""Creating a RMSE function to evaluate the model's performance."""

from sklearn.metrics import mean_squared_error
def rmse(y_test,y_pred):
      return np.sqrt(mean_squared_error(y_test,y_pred))

#Checking our predictions
y_pred = regr.predict(X2)

y_test = label_df

print("XGBoost score on training set: ", rmse(y_test, y_pred))

y_pred_xgb = regr.predict(test_new)

pred1 = pd.DataFrame({'Id': test['Id'], 'SalePrice': np.exp(y_pred_xgb)})
pred1.to_csv('xgbnono.csv', header=True, index=False)

#from google.colab import files
#files.download("xgbnono.csv")

from sklearn.linear_model import Lasso

#Alpha
alpha = 0.00099
#Can play around with this hyperparameter, found this to be the best one.

regr = Lasso(alpha=alpha,max_iter = 50000)
regr.fit(X2,label_df)

# run prediction on the training set to get a rough idea of how well it does
y_pred = regr.predict(X2)
y_test = label_df
print("Lasso score on training set: ", rmse(y_test, y_pred))

#make prediction on the test set
y_pred_lasso = regr.predict(test_new)
lasso_ex = np.exp(y_pred_lasso)
pred1 = pd.DataFrame({'Id': test['Id'], 'SalePrice': lasso_ex})
pred1.to_csv('lasso_model.csv', header=True, index=False)

#from google.colab import files
#files.download("lasso_model.csv")

from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.preprocessing import StandardScaler

np.random.seed(10)

#create Model
#define base model
def base_model():
     model = Sequential()
     model.add(Dense(20, input_dim=399, init='normal', activation='relu'))
     model.add(Dense(10, init='normal', activation='relu'))
     model.add(Dense(1, init='normal'))
     model.compile(loss='mean_squared_error', optimizer = 'adam')
     return model

seed = 7
np.random.seed(seed)

scale = StandardScaler()
X_train = scale.fit_transform(X2)
X_test = scale.fit_transform(test_new)

keras_label = label_df.as_matrix()
clf = KerasRegressor(build_fn=base_model, nb_epoch=1000, batch_size=5,verbose=0)
clf.fit(X_train,keras_label)

#make predictions and create the submission file 
kpred = clf.predict(X_test) 
kpred = np.exp(kpred)
pred_df = pd.DataFrame(kpred, index=test["Id"], columns=["SalePrice"]) 
pred_df.to_csv('keras1.csv', header=True, index_label='Id')

#from google.colab import files
#files.download("keras1.csv")

#simple average
y_pred = (y_pred_xgb + y_pred_lasso) / 2
y_pred = np.exp(y_pred)
pred_df = pd.DataFrame(y_pred, index=test["Id"], columns=["SalePrice"])
pred_df.to_csv('ensemble1.csv', header=True, index_label='Id')

#from google.colab import files
#files.download("ensemble1.csv")

